{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the problem parameters\n",
    "MAX_OFFER = 10  # Maximum possible offer\n",
    "MAINTENANCE_COST = 0.1  # Maintenance cost per day\n",
    "OFFER_PROBABILITIES = [0.0, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.3, 0.1]  # Probability of receiving each offer\n",
    "NUM_OFFERS = len(OFFER_PROBABILITIES)  # Number of distinct offers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_transition_matrix(offer_probabilities: list[float], max_offer: int) -> list[list[float]]:\n",
    "    transition_probability_matrix = [[0] * (max_offer + 1) for _ in range(max_offer + 1)]\n",
    "    for i in range(max_offer + 1):\n",
    "        for j in range(max_offer + 1):\n",
    "            if j < i:\n",
    "                transition_probability_matrix[i][j] = 0\n",
    "            elif j == i:\n",
    "                transition_probability_matrix[i][j] = sum(offer_probabilities[:min(j + 1, NUM_OFFERS)])\n",
    "            else:\n",
    "                if j < NUM_OFFERS:  # Ensure j is within the range of offer_probabilities\n",
    "                    transition_probability_matrix[i][j] = offer_probabilities[j]\n",
    "                else:\n",
    "                    transition_probability_matrix[i][j] = 0\n",
    "    return transition_probability_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma, theta=1e-6):\n",
    "    # Initialize the value function, Q-matrix, and policy\n",
    "    V = np.zeros(MAX_OFFER + 1)\n",
    "    Q = np.zeros((MAX_OFFER + 1, 2))  # 2 actions: 0 for continue, 1 for stop\n",
    "    policy = np.zeros(MAX_OFFER + 1, dtype=int)\n",
    "\n",
    "    # Initialize V and policy\n",
    "    for i in range(MAX_OFFER + 1):\n",
    "        V[i] = -i\n",
    "        policy[i] = 1\n",
    "\n",
    "    P = prob_transition_matrix(OFFER_PROBABILITIES, MAX_OFFER)\n",
    "    iter_num = 0\n",
    "\n",
    "    while True:\n",
    "        iter_num += 1\n",
    "        delta = 0\n",
    "\n",
    "        # Loop through all possible states (offers)\n",
    "        for state in range(MAX_OFFER + 1):\n",
    "            v = V[state]\n",
    "\n",
    "            # Calculate Q-values for both actions\n",
    "            # Action 0: Continue\n",
    "            Q[state, 0] = MAINTENANCE_COST + gamma * sum(P[state][offer] * V[min(offer, MAX_OFFER)] for offer in range(NUM_OFFERS))\n",
    "\n",
    "            # Action 1: Stop\n",
    "            Q[state, 1] = -state\n",
    "\n",
    "            # Update value function\n",
    "            V[state] = min(Q[state, 0], Q[state, 1])\n",
    "\n",
    "            # Update policy\n",
    "            policy[state] = np.argmin(Q[state])\n",
    "\n",
    "            # Update delta for convergence check\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "\n",
    "        # Check for convergence\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V, Q, policy, iter_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function: [ -3.0588235   -3.0588235   -3.0588235   -3.05882351  -4.\n",
      "  -5.          -6.          -7.          -8.          -9.\n",
      " -10.        ]\n",
      "Q Matrix: [[ -3.0588235    0.        ]\n",
      " [ -3.0588235   -1.        ]\n",
      " [ -3.0588235   -2.        ]\n",
      " [ -3.05882351  -3.        ]\n",
      " [ -3.2         -4.        ]\n",
      " [ -3.4         -5.        ]\n",
      " [ -3.625       -6.        ]\n",
      " [ -3.875       -7.        ]\n",
      " [ -4.15        -8.        ]\n",
      " [ -4.45        -9.        ]\n",
      " [ -4.9        -10.        ]]\n",
      "Policy: [0 0 0 0 1 1 1 1 1 1 1]\n",
      "Iterations: 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "v_value, Q, v_policy, num_iter = value_iteration(0.5)\n",
    "print(f\"Value Function:\", v_value)\n",
    "print(f\"Q Matrix:\", Q)\n",
    "print(f\"Policy:\", v_policy)\n",
    "print(\"Iterations:\", num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for gamma= 0.1: [ -0.5  -1.   -2.   -3.   -4.   -5.   -6.   -7.   -8.   -9.  -10. ] Policy for gamma= 0.1:[0 1 1 1 1 1 1 1 1 1 1] Iterations to converge: 2\n",
      "Value Function for gamma= 0.2: [ -1.1020408  -1.1020408  -2.         -3.         -4.         -5.\n",
      "  -6.         -7.         -8.         -9.        -10.       ] Policy for gamma= 0.2:[0 0 1 1 1 1 1 1 1 1 1] Iterations to converge: 4\n",
      "Value Function for gamma= 0.3: [ -1.72164947  -1.72164947  -2.          -3.          -4.\n",
      "  -5.          -6.          -7.          -8.          -9.\n",
      " -10.        ] Policy for gamma= 0.3:[0 0 1 1 1 1 1 1 1 1 1] Iterations to converge: 5\n",
      "Value Function for gamma= 0.4: [ -2.36956521  -2.36956521  -2.36956521  -3.          -4.\n",
      "  -5.          -6.          -7.          -8.          -9.\n",
      " -10.        ] Policy for gamma= 0.4:[0 0 0 1 1 1 1 1 1 1 1] Iterations to converge: 7\n",
      "Value Function for gamma= 0.5: [ -3.0588235   -3.0588235   -3.0588235   -3.05882351  -4.\n",
      "  -5.          -6.          -7.          -8.          -9.\n",
      " -10.        ] Policy for gamma= 0.5:[0 0 0 0 1 1 1 1 1 1 1] Iterations to converge: 8\n",
      "Value Function for gamma= 0.6: [ -3.82926812  -3.82926812  -3.82926812  -3.82926813  -4.\n",
      "  -5.          -6.          -7.          -8.          -9.\n",
      " -10.        ] Policy for gamma= 0.6:[0 0 0 0 1 1 1 1 1 1 1] Iterations to converge: 9\n",
      "Value Function for gamma= 0.7: [ -4.72222205  -4.72222205  -4.72222205  -4.72222205  -4.72222205\n",
      "  -5.          -6.          -7.          -8.          -9.\n",
      " -10.        ] Policy for gamma= 0.7:[0 0 0 0 0 1 1 1 1 1 1] Iterations to converge: 12\n",
      "Value Function for gamma= 0.8: [ -5.78124979  -5.78124979  -5.78124979  -5.78124979  -5.78124979\n",
      "  -5.78124983  -6.          -7.          -8.          -9.\n",
      " -10.        ] Policy for gamma= 0.8:[0 0 0 0 0 0 1 1 1 1 1] Iterations to converge: 15\n",
      "Value Function for gamma= 0.9: [ -7.10891042  -7.10891042  -7.10891042  -7.10891042  -7.10891042\n",
      "  -7.10891043  -7.10891046  -7.10891072  -8.          -9.\n",
      " -10.        ] Policy for gamma= 0.9:[0 0 0 0 0 0 0 0 1 1 1] Iterations to converge: 19\n",
      "Value Function for gamma= 1.0: [ -8.99999887  -8.99999887  -8.99999887  -8.99999887  -8.99999887\n",
      "  -8.99999887  -8.99999887  -8.99999888  -8.99999898  -9.\n",
      " -10.        ] Policy for gamma= 1.0:[0 0 0 0 0 0 0 0 0 0 1] Iterations to converge: 27\n"
     ]
    }
   ],
   "source": [
    "# Create a list of floats starting from 0.1 to 0.9 with step size 0.1\n",
    "list_of_gamma = [(0.1 * i).__round__(1) for i in range(1, 11)]\n",
    "\n",
    "for gamma in list_of_gamma:\n",
    "    v_value, Q, v_policy, num_iter = value_iteration(gamma)\n",
    "    print(f\"Value Function for gamma= {gamma}: {v_value}\", end=\" \")\n",
    "    print(f\"Policy for gamma= {gamma}:{v_policy}\", end=\" \")\n",
    "    print(f\"Iterations to converge: {num_iter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def policy_iteration(gamma, theta=1e-6):\n",
    "    # Initialize the value function and policy\n",
    "    V = np.zeros(MAX_OFFER + 1)\n",
    "    for i in range(MAX_OFFER+1):\n",
    "        V[i] = -i\n",
    "    policy = np.zeros(MAX_OFFER + 1, dtype=int)\n",
    "    for i in range(MAX_OFFER+1):\n",
    "        policy[i] = 1\n",
    "    # policy = np.random.randint(2, size=MAX_OFFER + 1)  # Initialize policy randomly\n",
    "    # print(policy)\n",
    "    # P = prob_transition_matrix(OFFER_PROBABILITIES, MAX_OFFER)\n",
    "    \n",
    "    iter_num = 0\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        iter_num+=1\n",
    "        # print(f\"At Iteration Number {iter_num} of Policy Iteration\")\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(MAX_OFFER + 1):\n",
    "                v = V[state]\n",
    "                \n",
    "                if policy[state] == 1:  # If the policy is to stop\n",
    "                    V[state] = -state\n",
    "                else:  # If the policy is to continue\n",
    "                    continue_value = gamma*sum(OFFER_PROBABILITIES[offer] * V[min(offer, MAX_OFFER)] for offer in range(NUM_OFFERS)) + MAINTENANCE_COST\n",
    "                    # print(continue_value)\n",
    "                    V[state] = continue_value\n",
    "                \n",
    "                delta = max(delta, abs(v - V[state]))\n",
    "            \n",
    "            # Check for convergence\n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "        policy_stable = True\n",
    "        \n",
    "        # Policy Improvement\n",
    "        for state in range(MAX_OFFER + 1):\n",
    "            old_action = policy[state]\n",
    "            optimal_action_value = -state  # Value if we stop selling\n",
    "            continue_value = gamma*sum(OFFER_PROBABILITIES[offer] * V[min(offer, MAX_OFFER)] for offer in range(NUM_OFFERS)) + MAINTENANCE_COST\n",
    "            \n",
    "            if continue_value < optimal_action_value:\n",
    "                policy[state] = 0  # Continue\n",
    "            else:\n",
    "                policy[state] = 1  # Stop\n",
    "            \n",
    "            # Check if the policy has changed\n",
    "            if old_action != policy[state]:\n",
    "                policy_stable = False\n",
    "        \n",
    "        # print(f\"Policy at iteration {iter_num}: {policy}\")\n",
    "        # If the policy is stable, we're done\n",
    "        if policy_stable:\n",
    "            break\n",
    "    \n",
    "    return V, policy, iter_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy Iteration:\n",
      "Value Function: [ -3.05882349  -3.05882349  -3.05882351  -3.05882352  -4.\n",
      "  -5.          -6.          -7.          -8.          -9.\n",
      " -10.        ]\n",
      "Policy: [0 0 0 0 1 1 1 1 1 1 1]\n",
      "Iterations: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPolicy Iteration:\")\n",
    "p_value, p_policy, num_iter = policy_iteration(0.5)\n",
    "print(\"Value Function:\", p_value)\n",
    "print(\"Policy:\", p_policy)\n",
    "print(\"Iterations:\", num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_OFFER = 10\n",
    "MAINTENANCE_COST = 0.1\n",
    "OFFER_PROBABILITIES = [0.0, 0.1, 0.1, 0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.3, 0.1]\n",
    "NUM_OFFERS = len(OFFER_PROBABILITIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for gamma= 0.1:[0 1 1 1 1 1 1 1 1 1 1] Iterations to converge: 2\n",
      "Policy for gamma= 0.2:[0 0 1 1 1 1 1 1 1 1 1] Iterations to converge: 2\n",
      "Policy for gamma= 0.3:[0 0 1 1 1 1 1 1 1 1 1] Iterations to converge: 2\n",
      "Policy for gamma= 0.4:[0 0 0 1 1 1 1 1 1 1 1] Iterations to converge: 2\n",
      "Policy for gamma= 0.5:[0 0 0 0 1 1 1 1 1 1 1] Iterations to converge: 3\n",
      "Policy for gamma= 0.6:[0 0 0 0 1 1 1 1 1 1 1] Iterations to converge: 2\n",
      "Policy for gamma= 0.7:[0 0 0 0 0 1 1 1 1 1 1] Iterations to converge: 2\n",
      "Policy for gamma= 0.8:[0 0 0 0 0 0 1 1 1 1 1] Iterations to converge: 3\n",
      "Policy for gamma= 0.9:[0 0 0 0 0 0 0 0 1 1 1] Iterations to converge: 3\n",
      "Policy for gamma= 1.0:[0 0 0 0 0 0 0 0 0 1 1] Iterations to converge: 3\n"
     ]
    }
   ],
   "source": [
    "# Create a list of floats starting from 0.1 to 0.9 with step size 0.1\n",
    "list_of_gamma = [(0.1 * i).__round__(1) for i in range(1, 11)]\n",
    "\n",
    "for gamma in list_of_gamma:\n",
    "    p_value, p_policy, num_iter = policy_iteration(gamma)\n",
    "    # print(f\"Value Function for gamma= {gamma}: {p_value}\", end=\" \")\n",
    "    print(f\"Policy for gamma= {gamma}:{p_policy}\", end=\" \")\n",
    "    print(f\"Iterations to converge: {num_iter}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
